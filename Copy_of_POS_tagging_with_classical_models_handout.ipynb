{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of POS_tagging_with_classical_models_handout.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no_k76e0NZHI"
      },
      "source": [
        "# Task: Part of speech tagging\n",
        "\n",
        "In this task we try to recreate a very rudimentary POS tagger \"from scratch\" using SpaCy and CRF models. \n",
        "\n",
        "(We disregard the fact, that SpaCy has a built in POS tagger for the moment for demonstration purposes.)\n",
        "\n",
        "The input is a tokenized English sentence. The task is to label each word with a part of speech (POS) tag. The tag set, which is identical the [Universal Dependencies project's](https://universaldependencies.org/) basic tag set is the following:\n",
        "\n",
        "- NOUN: noun\n",
        "- VERB: verb\n",
        "- DET: determiner\n",
        "- ADJ: adjective\n",
        "- ADP: adposition (e.g., prepositions)\n",
        "- ADV: adverb\n",
        "- CONJ: conjunction\n",
        "- NUM: numeral\n",
        "- PART: particle (function word that cannot be inflected, has no meaning in\n",
        "  itself and doesn't fit elsewhere, e.g., \"to\")\n",
        "- PRON: pronoun\n",
        "- .: punctuation\n",
        "- X: other\n",
        "\n",
        "The code in this task is an adaptation of the NER code in the sklearn-crfsuite documentation.\n",
        "\n",
        "# The data set\n",
        "\n",
        "__Brown__ corpus: \"The Brown University Standard Corpus of Present-Day American English (or just Brown Corpus) was compiled in the 1960s by Henry Kučera and W. Nelson Francis at Brown University, Providence, Rhode Island as a general corpus (text collection) in the field of corpus linguistics. **It contains 500 samples of English-language text, totaling roughly one million words, compiled from works published in the United States in 1961\"** (Wikpedia: Brown Corpus)\n",
        "\n",
        "Let's download and inspect the data!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VywNgkbfBWdS"
      },
      "source": [
        "%%capture\n",
        "!pip install nltk"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:32:20.712982Z",
          "start_time": "2019-11-12T09:32:16.093693Z"
        },
        "id": "KAPwh8mmNZHM",
        "outputId": "c07adf84-dd60-4f49-d38f-5fa17256b4b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.corpus import brown\n",
        "nltk.download('brown')\n",
        "\n",
        "brown.words()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:32:21.291370Z",
          "start_time": "2019-11-12T09:32:20.723897Z"
        },
        "id": "3kSgq4e0NZHi",
        "outputId": "fd994bd3-d9b2-40ff-ba32-a4297fff47ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('universal_tagset')\n",
        "brown.tagged_words(tagset='universal')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DET'), ('Fulton', 'NOUN'), ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:32:21.352042Z",
          "start_time": "2019-11-12T09:32:21.297210Z"
        },
        "id": "9tT1DBTtNZHu",
        "outputId": "139ebe9e-ec34-4892-98a3-fa38004f9ae0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "brown.sents()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:32:25.131849Z",
          "start_time": "2019-11-12T09:32:21.365202Z"
        },
        "id": "UuwBB-XRNZH6",
        "outputId": "8f7cef99-9557-4179-9263-771c1f5239b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(brown.words())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1161192"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8VYZmFCsNTa"
      },
      "source": [
        "From the brown the object provided by NLTK we will work with the tagged sentence list:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:32:25.163453Z",
          "start_time": "2019-11-12T09:32:25.136038Z"
        },
        "id": "bh0wAJkWNnlV",
        "outputId": "91984b41-3b7a-461b-a24a-266d22a984b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#For each sentence we have created a tuple of (word, POS_tag).\n",
        "sents = brown.tagged_sents(tagset=\"universal\")\n",
        "\n",
        "sents[:2]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('The', 'DET'),\n",
              "  ('Fulton', 'NOUN'),\n",
              "  ('County', 'NOUN'),\n",
              "  ('Grand', 'ADJ'),\n",
              "  ('Jury', 'NOUN'),\n",
              "  ('said', 'VERB'),\n",
              "  ('Friday', 'NOUN'),\n",
              "  ('an', 'DET'),\n",
              "  ('investigation', 'NOUN'),\n",
              "  ('of', 'ADP'),\n",
              "  (\"Atlanta's\", 'NOUN'),\n",
              "  ('recent', 'ADJ'),\n",
              "  ('primary', 'NOUN'),\n",
              "  ('election', 'NOUN'),\n",
              "  ('produced', 'VERB'),\n",
              "  ('``', '.'),\n",
              "  ('no', 'DET'),\n",
              "  ('evidence', 'NOUN'),\n",
              "  (\"''\", '.'),\n",
              "  ('that', 'ADP'),\n",
              "  ('any', 'DET'),\n",
              "  ('irregularities', 'NOUN'),\n",
              "  ('took', 'VERB'),\n",
              "  ('place', 'NOUN'),\n",
              "  ('.', '.')],\n",
              " [('The', 'DET'),\n",
              "  ('jury', 'NOUN'),\n",
              "  ('further', 'ADV'),\n",
              "  ('said', 'VERB'),\n",
              "  ('in', 'ADP'),\n",
              "  ('term-end', 'NOUN'),\n",
              "  ('presentments', 'NOUN'),\n",
              "  ('that', 'ADP'),\n",
              "  ('the', 'DET'),\n",
              "  ('City', 'NOUN'),\n",
              "  ('Executive', 'ADJ'),\n",
              "  ('Committee', 'NOUN'),\n",
              "  (',', '.'),\n",
              "  ('which', 'DET'),\n",
              "  ('had', 'VERB'),\n",
              "  ('over-all', 'ADJ'),\n",
              "  ('charge', 'NOUN'),\n",
              "  ('of', 'ADP'),\n",
              "  ('the', 'DET'),\n",
              "  ('election', 'NOUN'),\n",
              "  (',', '.'),\n",
              "  ('``', '.'),\n",
              "  ('deserves', 'VERB'),\n",
              "  ('the', 'DET'),\n",
              "  ('praise', 'NOUN'),\n",
              "  ('and', 'CONJ'),\n",
              "  ('thanks', 'NOUN'),\n",
              "  ('of', 'ADP'),\n",
              "  ('the', 'DET'),\n",
              "  ('City', 'NOUN'),\n",
              "  ('of', 'ADP'),\n",
              "  ('Atlanta', 'NOUN'),\n",
              "  (\"''\", '.'),\n",
              "  ('for', 'ADP'),\n",
              "  ('the', 'DET'),\n",
              "  ('manner', 'NOUN'),\n",
              "  ('in', 'ADP'),\n",
              "  ('which', 'DET'),\n",
              "  ('the', 'DET'),\n",
              "  ('election', 'NOUN'),\n",
              "  ('was', 'VERB'),\n",
              "  ('conducted', 'VERB'),\n",
              "  ('.', '.')]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:32:28.899336Z",
          "start_time": "2019-11-12T09:32:25.166107Z"
        },
        "id": "L2oUMrTpasZY",
        "outputId": "4b9e412c-addc-4592-a8df-259f8ecc7bc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# How many sentences are there in corpus.\n",
        "len(sents)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57340"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgvacV45sNTz"
      },
      "source": [
        "We divide our data set into a train and a valid part:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:32:28.905030Z",
          "start_time": "2019-11-12T09:32:28.901963Z"
        },
        "id": "jPvFic-atE6S"
      },
      "source": [
        "valid_sents = sents[:5734] # First 5734 sentences are validation sentences.\n",
        "train_sents = sents[5734:] # Remaining (57340 - 5734 = 51606 sents) are part of Train set."
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWUHCTttfmnS",
        "outputId": "e977022b-14cf-48eb-b8fc-a882e1da27b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Remove this afterwards.\n",
        "train_sents"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('Viewed', 'VERB'), ('from', 'ADP'), ('afar', 'ADV'), (',', '.'), ('the', 'DET'), ('CDC', 'NOUN'), ('looks', 'VERB'), ('like', 'ADP'), ('a', 'DET'), ('rather', 'ADV'), ('stalwart', 'ADJ'), ('political', 'ADJ'), ('pyramid', 'NOUN'), (':', '.'), ('its', 'DET'), ('elected', 'VERB'), ('directorate', 'NOUN'), ('fans', 'VERB'), ('out', 'PRT'), ('into', 'ADP'), ('an', 'DET'), ('array', 'NOUN'), ('of', 'ADP'), ('district', 'NOUN'), ('leaders', 'NOUN'), ('and', 'CONJ'), ('standing', 'VERB'), ('committees', 'NOUN'), (',', '.'), ('and', 'CONJ'), ('thence', 'ADV'), ('into', 'ADP'), ('its', 'DET'), ('component', 'NOUN'), ('clubs', 'NOUN'), ('and', 'CONJ'), ('affiliated', 'VERB'), ('groups', 'NOUN'), ('--', '.'), ('500', 'NUM'), ('or', 'CONJ'), ('so', 'ADV'), ('.', '.')], [('Much', 'ADJ'), ('of', 'ADP'), ('its', 'DET'), ('strength', 'NOUN'), ('stems', 'VERB'), ('from', 'ADP'), ('the', 'DET'), ('comfortable', 'ADJ'), ('knowledge', 'NOUN'), ('that', 'ADP'), ('every', 'DET'), ('``', '.'), ('volunteer', 'NOUN'), (\"''\", '.'), ('Democratic', 'ADJ'), ('organization', 'NOUN'), ('of', 'ADP'), ('any', 'DET'), ('consequence', 'NOUN'), ('belongs', 'VERB'), ('to', 'ADP'), ('the', 'DET'), ('Aj', 'NOUN'), ('.', '.')], ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO8hXBHHPR4f"
      },
      "source": [
        "# Feature template\n",
        "\n",
        "Since the plan is to build a CRF model, we need a __feature template__, which ***generates features for a word in a sentence*** (our sequence in the sequence tagging task). We use spaCy for feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:32:36.258620Z",
          "start_time": "2019-11-12T09:32:28.906793Z"
        },
        "id": "G-xzKye6BWeM"
      },
      "source": [
        "# We download the English language model for Spacy\n",
        "#Spacy install, load and such stuff\n",
        "#!python -m spacy download en_core_web_sm\n",
        "\n",
        "#Import\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "from spacy.tokens import Doc\n",
        "#By model load, please deactivate unnecessary pipeline elements!\n",
        "\n",
        "en = spacy.load('en_core_web_sm', disable=[\"ner\"])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyRTIKeB6bvO",
        "outputId": "c11cf869-5fde-4d49-df52-1d5792ed0aa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "en.pipeline"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7f79ba04da58>),\n",
              " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7f79b2914a68>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC4f13w9sNUJ"
      },
      "source": [
        "We write a function which generates features for a token in a sentence, which is already a spaCy document. The feature vector is represented as a `dict` mapping feature names to their values.\n",
        "\n",
        "The desired **feature set for a token is**:\n",
        "\n",
        "- `bias`: A constant value of 1 as an input\n",
        "- `token.lower`: the lowercased textual form of the token\n",
        "- `token.suffix`: the textual form of the token's suffix as defined by SpaCy,\n",
        "- `token.prefix`: the textual form of the token's prefix as defined by SpaCy,\n",
        "- `token.is_upper`: boolean value indicating if the token is uppercase,\n",
        "- `token.is_title`: boolean value indicating if the token is a title,\n",
        "- `token.is_digit`: boolean value indicating if the token consists of numbers.\n",
        "\n",
        "These are only the `Token`'s own properties, but they represent no context.\n",
        "\n",
        "We would like to include information about  the previous and next words, as well as indicating if the `Token` is the beginning or the end of sentence.\n",
        "\n",
        "The **contextual features** should be:\n",
        " \n",
        "- `-1:token.lower`: What is the lowercase textual form of the previous token?,\n",
        "- `-1:token.is_title`: Is the previous token a title?,\n",
        "- `-1:token.is_upper`: Is the previous token uppercase?,\n",
        "- `+1:token.lower`: What is the lowercase textual form of the next token?,\n",
        "- `+1:token.is_title`: Is the next token a title?,\n",
        "- `+1:token.is_upper`: Is the next token uppercase?,\n",
        "- `BOS`: Boolean value indicating if the token is the beginning of a sentence,\n",
        "- `EOS`: Boolean value indicating if the token is the end of a sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:32:37.930451Z",
          "start_time": "2019-11-12T09:32:37.909065Z"
        },
        "id": "SKz9zT8bsNUL"
      },
      "source": [
        "# in furtur we will send a sentence and its index \"i\" which indicate the word \n",
        "# in this function we create a dic for that ith word.\n",
        "def token2features(sent, i):\n",
        "    \"\"\"Return a feature dict for a token. \n",
        "    sent is a spaCy Doc containing a sentence, i is the token's index in it.\n",
        "    \"\"\"\n",
        "    features = {}\n",
        "    \n",
        "    features['bias'] = 1.0\n",
        "    features['token.lower'] = sent[i].lower_\n",
        "    features['token.suffix'] = sent[i].suffix_\n",
        "    features['token.prefix'] = sent[i].prefix_\n",
        "    features['token.is_upper'] = sent[i].is_upper\n",
        "    features['token.is_title'] = sent[i].is_title\n",
        "    features['token.is_digit'] = sent[i].is_digit\n",
        "    \n",
        "    # we dont have a token before the first token i.e. token at index=0\n",
        "    # Hence dont create the folowing features for it\n",
        "    if i>0:\n",
        "      features['-1:token.lower'] = sent[i-1].lower_\n",
        "      features['-1:token.is_title'] = sent[i-1].is_title\n",
        "      features['-1:token.is_upper'] = sent[i-1].is_upper\n",
        "\n",
        "    # we dont have a token after the last token i.e. token at index=len(sent)-1\n",
        "    # Hence dont create the folowing features for i>=len(sent)-1\n",
        "    if i<len(sent)-1:\n",
        "      features['+1:token.lower'] = sent[i+1].lower_\n",
        "      features['+1:token.is_title'] = sent[i+1].is_title\n",
        "      features['+1:token.is_upper:'] = sent[i+1].is_upper\n",
        "    \n",
        "    # if its BOS, i would be equal to 0,\n",
        "    # hence BOS=TRUE for that token\n",
        "    if i==0:\n",
        "      features['BOS'] = True\n",
        "    \n",
        "    # if its EOS, index i would be one less than \n",
        "    #len of the sentence (index starts from 0)\n",
        "    if i==len(sent)-1:\n",
        "      features['EOS'] = True\n",
        "    #features = [features]\n",
        "    return features"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvwL0hF3sNUS"
      },
      "source": [
        "For training, we will also need functions to generate feature dict and label lists for sentences in our training corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:32:37.958184Z",
          "start_time": "2019-11-12T09:32:37.936592Z"
        },
        "id": "ZLW80wtksNUU"
      },
      "source": [
        "def sent2features(sent):\n",
        "    \"Return a list of feature dicts for a sentence in the data set.\"\n",
        "    # Create a doc by instantiating a Doc class and iterating through the sentence token by token.\n",
        "    # Please bear in mind, that Brown has token-POS pairs, latter one we don't need here...\n",
        "\n",
        "    # get the tokens form token-POS pairs in \"sent\"\n",
        "    tokens = [token_POS_pair[0] for token_POS_pair in sent]\n",
        "    doc = Doc(en.vocab, tokens)\n",
        "  \n",
        "    # Plese use the above defined token2features function on each token to generate the features\n",
        "    # For the whole sentence!\n",
        "    sent_features=[] #create a list that stores feature dic\n",
        "    for i in range(len(tokens)): \n",
        "      sent_features.append(token2features(doc, i))\n",
        "\n",
        "    return sent_features\n",
        "\n",
        "def sent2labels(sent):\n",
        "    \n",
        "    #Please create / filter only the labels for given sentence!\n",
        "    labels = [token_POS_pair[1] for token_POS_pair in sent]\n",
        "    \n",
        "    return labels"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBoPuzeMsNUa"
      },
      "source": [
        "Sanity check: let's see the values for the first 2 tokens in the corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:32:37.997140Z",
          "start_time": "2019-11-12T09:32:37.966347Z"
        },
        "id": "UcqvDIJofccv",
        "outputId": "9d5c891a-73fc-4e2f-c62c-c9159b4004a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# This is for only first 2 words in the sentence\n",
        "print(sent2features(sents[0])[:2])\n",
        "print(sent2labels(sents[0])[:2])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'bias': 1.0, 'token.lower': 'the', 'token.suffix': 'The', 'token.prefix': 'T', 'token.is_upper': False, 'token.is_title': True, 'token.is_digit': False, '+1:token.lower': 'fulton', '+1:token.is_title': True, '+1:token.is_upper:': False, 'BOS': True}, {'bias': 1.0, 'token.lower': 'fulton', 'token.suffix': 'ton', 'token.prefix': 'F', 'token.is_upper': False, 'token.is_title': True, 'token.is_digit': False, '-1:token.lower': 'the', '-1:token.is_title': True, '-1:token.is_upper': False, '+1:token.lower': 'county', '+1:token.is_title': True, '+1:token.is_upper:': False}]\n",
            "['DET', 'NOUN']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsoK0-GNfzt5"
      },
      "source": [
        "# Putting the data into final form"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ylfst7VGsNUl"
      },
      "source": [
        "Everything is ready to generate the training data in the form which is usable for the CRFsuite. Note that our inputs and labels will be  2-level representations, lists of lists, because we deal with token sequences (sentences)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:33:02.066506Z",
          "start_time": "2019-11-12T09:32:38.005545Z"
        },
        "id": "Wfqa0feYgspT",
        "outputId": "8ac0272d-982a-4d57-8d6a-083da84c8af7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "X_train = [sent2features(s) for s in train_sents]\n",
        "y_train = [sent2labels(s) for s in train_sents]\n",
        "\n",
        "X_valid = [sent2features(s) for s in valid_sents]\n",
        "y_valid = [sent2labels(s) for s in valid_sents]"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 20 s, sys: 2.97 s, total: 23 s\n",
            "Wall time: 23 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:33:02.072026Z",
          "start_time": "2019-11-12T09:33:02.068258Z"
        },
        "id": "XNFvu0UosNUt",
        "outputId": "e6940b97-7914-4a3d-f411-b4648d538fb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Feature dict for the first token in the first validation sentence:\")\n",
        "print(X_valid[0][0])\n",
        "print(\"Its label:\")\n",
        "print(y_valid[0][0])"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature dict for the first token in the first validation sentence:\n",
            "{'bias': 1.0, 'token.lower': 'the', 'token.suffix': 'The', 'token.prefix': 'T', 'token.is_upper': False, 'token.is_title': True, 'token.is_digit': False, '+1:token.lower': 'fulton', '+1:token.is_title': True, '+1:token.is_upper:': False, 'BOS': True}\n",
            "Its label:\n",
            "DET\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2siQxe9k4ql"
      },
      "source": [
        "# Training and evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwyn356ysNU2"
      },
      "source": [
        "We use the super-optimized [CRFsuite](http://www.chokkan.org/software/crfsuite/) via the scikit-learn compatible [sklearn-crfsuite](https://sklearn-crfsuite.readthedocs.io) wrapper to train a CRF model on the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:33:04.830327Z",
          "start_time": "2019-11-12T09:33:02.073675Z"
        },
        "id": "15POzt86sNSe",
        "outputId": "f07e5685-d252-400e-a5de-499654008720",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%capture # only to avoid ugly printouts during install\n",
        "!pip install sklearn_crfsuite"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UsageError: unrecognized arguments: only to avoid ugly printouts during install\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:33:04.883741Z",
          "start_time": "2019-11-12T09:33:04.836395Z"
        },
        "id": "WkX57BFDklEL"
      },
      "source": [
        "# Please import and train an averaged perceptron model from CRFsuite and use it's custom metrics,\n",
        "# especially the multiple forms of accuracy score to evaluate the model! \n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import metrics\n",
        "from sklearn_crfsuite import scorers"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESDAKUI51Vcz"
      },
      "source": [
        "**Define the Model and then train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTJ1VTvY1U7e",
        "outputId": "18fc1544-c397-403c-bf1f-c28048b7fb1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "# Create the model\n",
        "crf = sklearn_crfsuite.CRF(algorithm='ap',max_iterations=23,verbose=True)\n",
        "# Train the model\n",
        "crf.fit(X_train, y_train)"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading training data to CRFsuite: 100%|██████████| 51606/51606 [00:13<00:00, 3914.93it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Feature generation\n",
            "type: CRF1d\n",
            "feature.minfreq: 0.000000\n",
            "feature.possible_states: 0\n",
            "feature.possible_transitions: 0\n",
            "0....1....2....3....4....5....6....7....8....9....10\n",
            "Number of features: 276114\n",
            "Seconds required: 2.533\n",
            "\n",
            "Averaged perceptron\n",
            "max_iterations: 23\n",
            "epsilon: 0.000000\n",
            "\n",
            "Iter 1   time=1.72  loss=3419.61  feature_norm=479.36\n",
            "Iter 2   time=1.59  loss=2119.91  feature_norm=605.32\n",
            "Iter 3   time=1.54  loss=1803.85  feature_norm=696.98\n",
            "Iter 4   time=1.52  loss=1613.51  feature_norm=771.06\n",
            "Iter 5   time=1.51  loss=1479.13  feature_norm=834.37\n",
            "Iter 6   time=1.48  loss=1373.25  feature_norm=890.05\n",
            "Iter 7   time=1.47  loss=1306.41  feature_norm=940.22\n",
            "Iter 8   time=1.46  loss=1234.49  feature_norm=985.92\n",
            "Iter 9   time=1.47  loss=1164.34  feature_norm=1028.09\n",
            "Iter 10  time=1.45  loss=1131.19  feature_norm=1067.25\n",
            "Iter 11  time=1.44  loss=1085.22  feature_norm=1103.89\n",
            "Iter 12  time=1.52  loss=1060.18  feature_norm=1138.42\n",
            "Iter 13  time=1.39  loss=1006.77  feature_norm=1170.95\n",
            "Iter 14  time=1.38  loss=992.93   feature_norm=1201.74\n",
            "Iter 15  time=1.42  loss=967.51   feature_norm=1231.09\n",
            "Iter 16  time=1.36  loss=931.00   feature_norm=1259.12\n",
            "Iter 17  time=1.36  loss=923.85   feature_norm=1285.93\n",
            "Iter 18  time=1.36  loss=891.74   feature_norm=1311.65\n",
            "Iter 19  time=1.36  loss=866.99   feature_norm=1336.30\n",
            "Iter 20  time=1.35  loss=863.92   feature_norm=1359.99\n",
            "Iter 21  time=1.35  loss=867.29   feature_norm=1382.89\n",
            "Iter 22  time=1.35  loss=835.07   feature_norm=1405.04\n",
            "Iter 23  time=1.33  loss=823.57   feature_norm=1426.49\n",
            "Total seconds required for training: 33.170\n",
            "\n",
            "Storing the model\n",
            "Number of active features: 118686 (276114)\n",
            "Number of active attributes: 68711 (142416)\n",
            "Number of active labels: 12 (12)\n",
            "Writing labels\n",
            "Writing attributes\n",
            "Writing feature references for transitions\n",
            "Writing feature references for attributes\n",
            "Seconds required: 0.073\n",
            "\n",
            "CPU times: user 48.9 s, sys: 277 ms, total: 49.2 s\n",
            "Wall time: 49 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-5wheSj3dtZ"
      },
      "source": [
        "**Predict on X_valid**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SWhZmOK2ToD"
      },
      "source": [
        "y_pred = crf.predict(X_valid)"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma3Hxao3Jx0E"
      },
      "source": [
        "**Check the performance on Validation set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY8zz92Y3qLO",
        "outputId": "51d89991-dc90-4622-87f3-b4b349e40af3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(metrics.flat_classification_report(y_valid, y_pred, labels=None, digits=3))"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           .      1.000     1.000     1.000     14377\n",
            "         ADJ      0.913     0.925     0.919      8525\n",
            "         ADP      0.982     0.984     0.983     15138\n",
            "         ADV      0.933     0.940     0.936      4438\n",
            "        CONJ      0.993     0.998     0.996      3435\n",
            "         DET      0.997     0.996     0.996     14128\n",
            "        NOUN      0.980     0.974     0.977     36341\n",
            "         NUM      0.985     0.984     0.984      2386\n",
            "        PRON      0.988     0.995     0.992      3427\n",
            "         PRT      0.945     0.940     0.943      2877\n",
            "        VERB      0.976     0.978     0.977     18229\n",
            "           X      0.851     0.570     0.683       100\n",
            "\n",
            "    accuracy                          0.977    123401\n",
            "   macro avg      0.962     0.940     0.949    123401\n",
            "weighted avg      0.977     0.977     0.977    123401\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJH4pQSV8HtZ"
      },
      "source": [
        "Precision: Out of all the tags we predicted label=True, what fraction actually were True. \n",
        "\n",
        "Recall: Out of all the tags that actually had label=True, what fraction did we correctly predict as label=True.\n",
        "\n",
        "F1-Score: Weighted average of Precision and Recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7gKOgDaMPMt"
      },
      "source": [
        "Getting a low F1-Score for X tag. It's okay to check the skewness of labels in the data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUPjMijv8Pm1"
      },
      "source": [
        "Verb=dot=ADJ=ADP=ADV=CONJ=DET=NOUN=NUM=PRON=PRT=X=0\n",
        "counter=0\n",
        "for i in range(len(train_sents)):\n",
        "  POS_tags = [POS[1] for POS in train_sents[i]]\n",
        "  counter += len(POS_tags)\n",
        "  Verb += POS_tags.count(\"VERB\")\n",
        "  dot += POS_tags.count(\".\")\n",
        "  ADJ += POS_tags.count(\"ADJ\")\n",
        "  ADP += POS_tags.count(\"ADP\")\n",
        "  ADV += POS_tags.count(\"ADV\")\n",
        "  CONJ += POS_tags.count(\"CONJ\")\n",
        "  DET += POS_tags.count(\"DET\")\n",
        "  NOUN += POS_tags.count(\"NOUN\")\n",
        "  NUM += POS_tags.count(\"NUM\")\n",
        "  PRON += POS_tags.count(\"PRON\")\n",
        "  PRT += POS_tags.count(\"PRT\")\n",
        "  X += POS_tags.count(\"X\")"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id8yWd9qC3SG",
        "outputId": "e32ba978-c021-4e37-999d-225f3308ebc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Percentage of POS Tags in Train set\")\n",
        "print(\"Verbs:      \"+str(round(Verb/counter*100, 2)), \n",
        "      \"\\nPuntuations: \"+str(round(dot/counter*100, 2)),\n",
        "      \"\\nADJs:       \"+str(round(ADJ/counter*100, 2)), \n",
        "      \"\\nADPs:       \"+str(round(ADP/counter*100, 2)),\n",
        "      \"\\nADVs:       \"+str(round(ADV/counter*100, 2)), \n",
        "      \"\\nCONJs:      \"+str(round(CONJ/counter*100, 2)), \n",
        "      \"\\nDETs:       \"+str(round(DET/counter*100, 2)),\n",
        "      \"\\nNOUNs:      \"+str(round(NOUN/counter*100, 2)), \n",
        "      \"\\nNUMs:       \"+str(round(NUM/counter*100, 2)),\n",
        "      \"\\nPRONs:      \"+str(round(PRON/counter*100, 2)), \n",
        "      \"\\nPRTs:       \"+str(round(PRT/counter*100, 2)),\n",
        "      \"\\nXs:         \"+str(round(X/counter*100, 2))\n",
        "      )"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of POS Tags in Train set\n",
            "Verbs:      15.85 \n",
            "Puntuations: 12.83 \n",
            "ADJs:       7.25 \n",
            "ADPs:       12.49 \n",
            "ADVs:       4.99 \n",
            "CONJs:      3.35 \n",
            "DETs:       11.84 \n",
            "NOUNs:      23.05 \n",
            "NUMs:       1.2 \n",
            "PRONs:      4.42 \n",
            "PRTs:       2.6 \n",
            "Xs:         0.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyEgN-wuDG2n"
      },
      "source": [
        "It can be seen that we only have 0.12% of X tags, Hence we get poor F1-Score for this particular tag. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gMuLZLBUAAO"
      },
      "source": [
        "Overall the model performed really well. Except for the X tag."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63p9RtDhsNU_"
      },
      "source": [
        "Let's instantiate and fit our model. CRFsuite implements several learning methods, here we use \"ap\", i.e., averaged perceptron."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "li8CXg67sNVc"
      },
      "source": [
        "# Demonstration\n",
        "\n",
        "Just for the fun, we can try out the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:35:17.983727Z",
          "start_time": "2019-11-12T09:35:17.965723Z"
        },
        "id": "JHoYAGHFsNVe"
      },
      "source": [
        "def predict_tags(sent):\n",
        "    \"\"\"Predict tags for a sentence.\n",
        "    sent is a string.\n",
        "    \"\"\"\n",
        "    doc = en(sent)\n",
        "    return crf.predict([[token2features(doc, i) for i in range(len(doc))]])\n",
        "    "
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:35:37.676093Z",
          "start_time": "2019-11-12T09:35:17.986500Z"
        },
        "id": "Ya59xso_z7uj",
        "outputId": "cc2fdaf7-03a1-4edc-bb63-f5594a5ecd65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        }
      },
      "source": [
        " while True:\n",
        "        sent = input(\"\\nEnter a sentence to tag or press return to quit:\\n\")\n",
        "        if sent:\n",
        "            print(predict_tags(sent))\n",
        "        else:\n",
        "            print(\"\\nEmpty input received -- bye!\")\n",
        "            break"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Enter a sentence to tag or press return to quit:\n",
            "My name is Asjad. I love Data Science.\n",
            "[['DET', 'NOUN', 'VERB', 'NOUN', '.', 'PRON', 'VERB', 'NOUN', 'NOUN', '.']]\n",
            "\n",
            "Enter a sentence to tag or press return to quit:\n",
            "?\n",
            "[['.']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \"\"\"\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-96-6e9683f3a8fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m        \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEnter a sentence to tag or press return to quit:\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m            \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}